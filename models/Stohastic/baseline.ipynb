{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6823d13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip -q install hmmlearn arch statsmodels pandas numpy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b1ebd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from hmmlearn.hmm import GaussianHMM\n",
    "from arch import arch_model\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (12,5)\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ab2137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= ПАРАМЕТРЫ =========\n",
    "H            = 3        # ваш горизонт (как в таргете)\n",
    "BASE_THR     = 0.53     # базовый порог уверенности proba_up\n",
    "MARGIN_HIT   = 0.05     # на сколько p_hit_tp должно превосходить p_hit_sl\n",
    "VOL_PCTL_ABS = 0.95     # пратежка: не торговать в топ-5% волатильности\n",
    "TARGET_VOL   = 0.02     # дневной таргет под волатильность (пример)\n",
    "F_MAX        = 0.02     # максимум доля капитала (ограничитель Келли)\n",
    "TP_ATR_K     = 2      # базовый множитель ATR на TP (как в baseline)\n",
    "SL_ATR_K     = 1      # базовый множитель ATR на SL (как в baseline)\n",
    "# =============================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc02b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_atr_wilder(df, n=14):\n",
    "    \"\"\"True Range + Wilder's ATR, если нет atr_14 в CSV.\"\"\"\n",
    "    high = df['high'].astype(float)\n",
    "    low  = df['low'].astype(float)\n",
    "    close= df['close'].astype(float)\n",
    "    prev_close = close.shift(1)\n",
    "\n",
    "    tr1 = (high - low).abs()\n",
    "    tr2 = (high - prev_close).abs()\n",
    "    tr3 = (low  - prev_close).abs()\n",
    "    tr  = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)\n",
    "\n",
    "    atr = tr.ewm(alpha=1/n, adjust=False).mean()\n",
    "    return atr\n",
    "\n",
    "def fit_hmm_regimes(returns, n_states=2):\n",
    "    X = returns.dropna().values.reshape(-1,1)\n",
    "    if len(X) < 200:  # минимально, чтобы стабильно\n",
    "        return None, None\n",
    "    hmm = GaussianHMM(n_components=n_states, covariance_type='full',\n",
    "                      n_iter=200, random_state=42)\n",
    "    hmm.fit(X)\n",
    "    hidden = pd.Series(hmm.predict(X), index=returns.dropna().index)\n",
    "    return hmm, hidden.reindex(returns.index)\n",
    "\n",
    "def garch_sigma(returns):\n",
    "    # Еженаборная оценка σ_t через (E)GARCH: используем простой GARCH(1,1)\n",
    "    r = (returns.dropna()*100).astype(float)  # проценты\n",
    "    if len(r) < 300:\n",
    "        return returns.abs().rolling(20).std().reindex(returns.index)\n",
    "    am = arch_model(r, p=1, q=1, mean='Constant', vol='GARCH', dist='normal')\n",
    "    res = am.fit(disp=\"off\")\n",
    "    cond_vol = res.conditional_volatility / 100.0  # обратно в доли\n",
    "    return cond_vol.reindex(returns.index)\n",
    "\n",
    "def arima_sign(returns):\n",
    "    # Одношаговый ARIMA sanity-check (легкий)\n",
    "    y = returns.dropna()\n",
    "    if len(y) < 100:\n",
    "        return pd.Series(index=returns.index, dtype=float)\n",
    "    model = ARIMA(y, order=(1,0,1))\n",
    "    res = model.fit()\n",
    "    fc = res.forecast(1)\n",
    "    sgn = np.sign(fc.iloc[0])\n",
    "    return pd.Series(sgn, index=returns.index).ffill()\n",
    "\n",
    "def mc_hit_probs(close_series, mu, sigma, H, n_paths=2000, dt=1):\n",
    "    # Упрощенный GBM для hit-prob: вернём массив максимумов/минимумов за H шагов\n",
    "    S0 = close_series.iloc[-1]\n",
    "    z = np.random.normal(size=(n_paths, H))\n",
    "    steps = (mu - 0.5*sigma**2)*dt + sigma*np.sqrt(dt)*z\n",
    "    paths = S0 * np.exp(np.cumsum(steps, axis=1))\n",
    "    max_arr = paths.max(axis=1)\n",
    "    min_arr = paths.min(axis=1)\n",
    "    return max_arr, min_arr\n",
    "\n",
    "def kelly_lite(prob_up, R=1.0, f_max=F_MAX):\n",
    "    p = np.clip(prob_up, 1e-6, 1-1e-6)\n",
    "    f = (p*R - (1-p))/R\n",
    "    return float(np.clip(f, 0.0, f_max))\n",
    "\n",
    "def overlays_postprocess(df):\n",
    "    \"\"\"\n",
    "    Apply stochastic filters (HMM, GARCH, ARIMA, Monte Carlo) to generate long/short signals.\n",
    "    Expects DataFrame with columns time, close, high, low, and either proba_up/proba_down or pred_class.\n",
    "    Adds columns: signal (+1 long, -1 short, 0 skip), tp_price, sl_price, size.\n",
    "    \"\"\"\n",
    "    # Sort by time and copy to avoid modifying original\n",
    "    df = df.sort_values('time').reset_index(drop=True).copy()\n",
    "\n",
    "    # Ensure required price columns exist\n",
    "    assert {'close','high','low'}.issubset(df.columns), \"Нужны колонки close/high/low\"\n",
    "\n",
    "    # Must have either proba_up or pred_class to derive probabilities\n",
    "    if ('proba_up' not in df.columns) and ('pred_class' not in df.columns):\n",
    "        raise ValueError(\"Добавьте в входной набор либо 'proba_up', либо 'pred_class' из вашей модели.\")\n",
    "\n",
    "    # If ATR is missing, compute it via Wilder method\n",
    "    if 'atr_14' not in df.columns:\n",
    "        df['atr_14'] = compute_atr_wilder(df, n=14)\n",
    "\n",
    "    # Compute log returns for volatility/regime detection\n",
    "    df['logret'] = np.log(df['close']).diff()\n",
    "\n",
    "    # 1) Hidden Markov regime classification on returns\n",
    "    hmm, regimes = fit_hmm_regimes(df['logret'])\n",
    "    df['regime'] = regimes  # 0/1; if None remains NaN\n",
    "\n",
    "    # 2) Estimate conditional volatility σ_t via GARCH\n",
    "    df['sigma_t'] = garch_sigma(df['logret'])\n",
    "    vol_cut = df['sigma_t'].quantile(VOL_PCTL_ABS)\n",
    "\n",
    "    # 3) Quick ARIMA one-step sign estimate (sanity check)\n",
    "    df['arima_sign'] = arima_sign(df['logret'])\n",
    "\n",
    "    # Build probability columns: if proba_up missing, derive from pred_class\n",
    "    if 'proba_up' not in df.columns:\n",
    "        df['proba_up'] = (df['pred_class'] == 2).astype(float) * 0.6 + 0.2\n",
    "    # For symmetric trading we also need proba_down; if absent derive from pred_class or complement\n",
    "    if 'proba_down' not in df.columns:\n",
    "        if 'pred_class' in df.columns:\n",
    "            df['proba_down'] = (df['pred_class'] == 0).astype(float) * 0.6 + 0.2\n",
    "        else:\n",
    "            df['proba_down'] = 1.0 - df['proba_up']\n",
    "\n",
    "    # Initialize outputs: signal (+1 long, -1 short, 0 skip), take-profit price, stop-loss price, position size\n",
    "    out = []\n",
    "    for i in range(len(df)):\n",
    "        row = df.iloc[i]\n",
    "        # Skip initial rows where features are insufficient\n",
    "        if i < 50 or (row['logret'] != row['logret']):  # NaN check\n",
    "            out.append((0, float('nan'), float('nan'), 0.0))\n",
    "            continue\n",
    "\n",
    "        # Skip trading in extreme volatility regimes\n",
    "        if (row['sigma_t'] == row['sigma_t']) and row['sigma_t'] >= vol_cut:\n",
    "            out.append((0, float('nan'), float('nan'), 0.0))\n",
    "            continue\n",
    "\n",
    "        # Determine threshold (higher in high-vol regime)\n",
    "        thr = BASE_THR\n",
    "        if (row['regime'] == row['regime']) and int(row['regime']) == 1:\n",
    "            thr = BASE_THR + 0.05\n",
    "\n",
    "        # Candidate directions based on probabilities\n",
    "        long_cond = (row['proba_up']   >= thr)\n",
    "        short_cond= (row['proba_down'] >= thr)\n",
    "\n",
    "        # If neither direction meets threshold, skip\n",
    "        if not long_cond and not short_cond:\n",
    "            out.append((0, float('nan'), float('nan'), 0.0))\n",
    "            continue\n",
    "\n",
    "        # ARIMA sanity check: require ARIMA sign to match direction when confidence is low\n",
    "        # For long trades, a negative ARIMA sign suggests skip; for short trades, positive sign suggests skip\n",
    "        direction = None\n",
    "        if long_cond and not short_cond:\n",
    "            # long candidate\n",
    "            if (row['arima_sign'] == row['arima_sign']) and (row['arima_sign'] < 0) and (row['proba_up'] < (thr + 0.05)):\n",
    "                out.append((0, float('nan'), float('nan'), 0.0))\n",
    "                continue\n",
    "            direction = 'long'\n",
    "        elif short_cond and not long_cond:\n",
    "            # short candidate\n",
    "            if (row['arima_sign'] == row['arima_sign']) and (row['arima_sign'] > 0) and (row['proba_down'] < (thr + 0.05)):\n",
    "                out.append((0, float('nan'), float('nan'), 0.0))\n",
    "                continue\n",
    "            direction = 'short'\n",
    "        else:\n",
    "            # Both directions meet threshold; choose the higher probability\n",
    "            if row['proba_up'] >= row['proba_down']:\n",
    "                if (row['arima_sign'] == row['arima_sign']) and (row['arima_sign'] < 0) and (row['proba_up'] < (thr + 0.05)):\n",
    "                    out.append((0, float('nan'), float('nan'), 0.0))\n",
    "                    continue\n",
    "                direction = 'long'\n",
    "            else:\n",
    "                if (row['arima_sign'] == row['arima_sign']) and (row['arima_sign'] > 0) and (row['proba_down'] < (thr + 0.05)):\n",
    "                    out.append((0, float('nan'), float('nan'), 0.0))\n",
    "                    continue\n",
    "                direction = 'short'\n",
    "\n",
    "        # Determine ATR and fallback sigma estimate\n",
    "        atr = row['atr_14']\n",
    "        sigma = row['sigma_t'] if (row['sigma_t'] == row['sigma_t']) else df['logret'].rolling(20).std().iloc[i]\n",
    "        # Step distance: maximum of ATR-based and volatility-based distance\n",
    "        step = max(TP_ATR_K * atr, 2.0 * sigma * row['close'])\n",
    "\n",
    "        # Compute take-profit and stop-loss depending on direction\n",
    "        if direction == 'long':\n",
    "            base_tp = row['close'] + step\n",
    "            base_sl = row['close'] - step * (SL_ATR_K / TP_ATR_K)\n",
    "        else:\n",
    "            base_tp = row['close'] - step\n",
    "            base_sl = row['close'] + step * (SL_ATR_K / TP_ATR_K)\n",
    "\n",
    "        # Monte Carlo hit probability filter: require TP to be sufficiently more likely than SL\n",
    "        if (sigma == sigma) and sigma > 0:\n",
    "            mu_loc = df['logret'].iloc[max(0, i-50):i].mean()\n",
    "            max_arr, min_arr = mc_hit_probs(df['close'].iloc[:i+1], mu=mu_loc, sigma=sigma, H=H, n_paths=1500)\n",
    "            if direction == 'long':\n",
    "                p_hit_tp = (max_arr >= base_tp).mean()\n",
    "                p_hit_sl = (min_arr <= base_sl).mean()\n",
    "            else:\n",
    "                p_hit_tp = (min_arr <= base_tp).mean()\n",
    "                p_hit_sl = (max_arr >= base_sl).mean()\n",
    "            if p_hit_tp < p_hit_sl + MARGIN_HIT:\n",
    "                out.append((0, float('nan'), float('nan'), 0.0))\n",
    "                continue\n",
    "\n",
    "        # Position sizing via Kelly criterion and volatility targeting\n",
    "        # Risk-reward ratio for kelly: absolute of TP/SL difference relative to stop\n",
    "        R = abs((base_tp - row['close']) / max(abs(row['close'] - base_sl), 1e-9))\n",
    "        prob = row['proba_up'] if direction == 'long' else row['proba_down']\n",
    "        f_kelly = kelly_lite(prob, R=R, f_max=F_MAX)\n",
    "        size_vol = TARGET_VOL / max(sigma, 1e-6) if (sigma == sigma) else F_MAX\n",
    "        size = float(np.clip(min(f_kelly, size_vol), 0.0, F_MAX))\n",
    "\n",
    "        # Determine signal sign\n",
    "        signal = 1 if direction == 'long' else -1\n",
    "        # Append results\n",
    "        out.append((signal, base_tp, base_sl, size))\n",
    "\n",
    "    # Assign computed columns back to DataFrame\n",
    "    df['signal']   = [o[0] for o in out]\n",
    "    df['tp_price'] = [o[1] for o in out]\n",
    "    df['sl_price'] = [o[2] for o in out]\n",
    "    df['size']     = [o[3] for o in out]\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb1e824",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "\n",
    "df = pd.read_csv(\"../../data/SBER_dataset_5m.csv\", parse_dates=[\"time\"])\n",
    "\n",
    "# Заглушка — случайная вероятность \"up\"\n",
    "# (позже заменишь на реальные вероятности модели)\n",
    "df[\"proba_up\"] = np.random.uniform(0.1, 0.9, len(df))\n",
    "df[\"proba_down\"] = 1.0 - df[\"proba_up\"]\n",
    "\n",
    "# Если у тебя есть метка target (например, -1/0/1)\n",
    "# можно добавить суррогатный pred_class:\n",
    "df[\"pred_class\"] = df[\"label\"].map({-1:0, 0:1, 1:2}).fillna(1).astype(int)\n",
    "\n",
    "df.to_csv(\"signals_raw.csv\", index=False)\n",
    "print(\"✅ Created signals_raw.csv (synthetic proba_up/down added)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565d498b",
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_CSV  = \"signals_raw.csv\"     # поменяйте путь\n",
    "OUT_CSV = \"orders_overlays.csv\" # куда сохранить"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31d5175",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_in = pd.read_csv(IN_CSV, parse_dates=['time'])\n",
    "df_out = overlays_postprocess(df_in)\n",
    "df_out.to_csv(OUT_CSV, index=False)\n",
    "print(\"Saved:\", OUT_CSV)\n",
    "\n",
    "df_out.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00479c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_out.copy()\n",
    "df = df.sort_values('time').reset_index(drop=True)\n",
    "\n",
    "# Доходности (по close→close)\n",
    "df['ret_1'] = df['close'].pct_change()\n",
    "\n",
    "# Простая схема: если signal==1, считаем, что \"держим позицию\" на следующий шаг\n",
    "df['signal_shift'] = df['signal'].shift(1).fillna(0)\n",
    "df['str_ret'] = df['signal_shift'] * df['ret_1']    # без плеча, для прикидки\n",
    "\n",
    "bh = (1+df['ret_1'].fillna(0)).cumprod()\n",
    "st = (1+df['str_ret'].fillna(0)).cumprod()\n",
    "\n",
    "plt.plot(df['time'], bh, label='Buy&Hold')\n",
    "plt.plot(df['time'], st, label='Overlay Strategy (no leverage)')\n",
    "plt.legend()\n",
    "plt.title('Cumulative Returns (toy check)')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
