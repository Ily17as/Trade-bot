{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13572165,"sourceType":"datasetVersion","datasetId":8621897}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install timm torch torchvision tqdm pandas pillow","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# baseline_cuda.py\n# requirements: timm, torch, torchvision, tqdm, pandas, pillow\nimport os, json, time\nfrom pathlib import Path\n\nimport torch\nimport timm\nfrom torch import nn, optim\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader, Dataset\nfrom PIL import Image\nimport pandas as pd\nfrom tqdm.auto import tqdm","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ----------------- CONFIG -----------------\nMODEL_DIR = Path(\"models\"); MODEL_DIR.mkdir(exist_ok=True)\nLABELS_CSV = \"/kaggle/input/sber0601-1001/labels.csv\"      # adjust if needed\nIMAGES_ROOT = \"/kaggle/input/sber0601-1001/SBER_images(2025-06-01 - 2025-10-01)\"\nMODEL_NAME = \"convnext_tiny\"              # use a light model for test; change if needed\nPRETRAINED = True                         # set False to avoid downloading weights\nBATCH_SIZE = 64                           # increase with GPU memory\nVAL_BATCH = 128\nNUM_EPOCHS = 10","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Device (CUDA preferred)\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", DEVICE)\nif DEVICE.type == \"cpu\":\n    print(\"Warning: CUDA not available. Script will run on CPU. For CUDA set up drivers and run on GPU.\")\n\n# ---- load labels csv and build label_map ----\ndf = pd.read_csv(LABELS_CSV)\ndf['label'] = df['label'].astype(str).str.strip().str.lower()\nunique_labels = sorted(df['label'].unique())\nlabel_to_idx = {lab: idx for idx, lab in enumerate(unique_labels)}\ndf['label_idx'] = df['label'].map(label_to_idx).astype(int)\ndf['filename'] = df['filename'].apply(lambda x: os.path.join(IMAGES_ROOT, x))\n\n# quick sanity checks\nmissing = (~df['filename'].apply(os.path.exists)).sum()\nif missing:\n    print(f\"Warning: {missing} image paths not found. First missing (if any):\")\n    print(df.loc[~df['filename'].apply(os.path.exists), 'filename'].head(3))\nelse:\n    print(\"All image paths exist (quick check).\")\n\n# save meta\nmeta = {\"label_to_idx\": label_to_idx, \"input_size\": [3, 224, 224], \"model_name\": MODEL_NAME}\nwith open(MODEL_DIR / \"meta.json\", \"w\") as f:\n    json.dump(meta, f, indent=2)","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ---- Dataset ----\nclass ImgDataset(Dataset):\n    def __init__(self, df, transform):\n        self.df = df.reset_index(drop=True)\n        self.transform = transform\n    def __len__(self): return len(self.df)\n    def __getitem__(self, i):\n        p = self.df.loc[i, 'filename']\n        if not os.path.exists(p):\n            raise FileNotFoundError(f\"Image not found: {p}\")\n        img = Image.open(p).convert('RGB')\n        x = self.transform(img)\n        y = int(self.df.loc[i, 'label_idx'])\n        return x, torch.tensor(y, dtype=torch.long)","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.Resize((224,224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225))\n])\n\ntrain_df = df.sample(frac=0.8, random_state=42)\nval_df = df.drop(train_df.index)\n\n# DataLoader: pin_memory=True speeds host->GPU transfer. num_workers>0 recommended on Linux. \ntrain_dl = DataLoader(ImgDataset(train_df, transform),\n                      batch_size=BATCH_SIZE, shuffle=True,\n                      pin_memory=(DEVICE.type=='cuda'))\nval_dl   = DataLoader(ImgDataset(val_df, transform),\n                      batch_size=VAL_BATCH, shuffle=False,\n                      pin_memory=(DEVICE.type=='cuda'))","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Reconfigure DataLoader with workers\nnum_workers = 4 if DEVICE.type == 'cuda' else 0\npersistent_workers = (num_workers > 0)\ntrain_dl = DataLoader(\n    ImgDataset(train_df, transform),\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    num_workers=num_workers,\n    persistent_workers=persistent_workers,\n    pin_memory=(DEVICE.type == 'cuda')\n)\nval_dl = DataLoader(\n    ImgDataset(val_df, transform),\n    batch_size=VAL_BATCH,\n    shuffle=False,\n    num_workers=num_workers,\n    persistent_workers=persistent_workers,\n    pin_memory=(DEVICE.type == 'cuda')\n)\n","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- create backbone (diagnostics) ---\nprint(\"Creating model. If pretrained=True this may download weights.\")\nt0 = time.time()\ntry:\n    backbone = timm.create_model(MODEL_NAME, pretrained=PRETRAINED, num_classes=0, global_pool='avg')\nexcept Exception as e:\n    print(\"Model creation failed:\", e)\n    print(\"Retrying with pretrained=False ...\")\n    backbone = timm.create_model(MODEL_NAME, pretrained=False, num_classes=0, global_pool='avg')\nt1 = time.time()\nprint(f\"Model created in {t1-t0:.1f}s. num_features={backbone.num_features}\")\n\nfeat_dim = backbone.num_features\nfor p in backbone.parameters(): p.requires_grad = False\n\nhead = nn.Sequential(\n    nn.Linear(feat_dim, 512),\n    nn.ReLU(),\n    nn.Dropout(0.3),\n    nn.Linear(512, len(label_to_idx))\n)\n\nmodel = nn.Sequential(backbone, head).to(DEVICE)\n\n# optimizer and loss\nopt = optim.Adam(head.parameters(), lr=3e-4, weight_decay=1e-4)\nclass_weights = torch.ones(len(label_to_idx), dtype=torch.float32).to(DEVICE)\ncriterion = nn.CrossEntropyLoss(weight=class_weights)\n\n# mixed precision (use only if device is cuda)\nuse_amp = (DEVICE.type == \"cuda\")\nscaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# training loop (uses non_blocking=True when moving tensors to GPU)\nbest_val = -1.0\nfor epoch in range(NUM_EPOCHS):\n    model.train()\n    train_loss = 0.0\n    n_samples = 0\n    pbar = tqdm(train_dl, desc=f\"Epoch {epoch:02d} [train]\", leave=False)\n    for xb, yb in pbar:\n        # Move to device. use non_blocking if pinned memory is enabled.\n        xb = xb.to(DEVICE, non_blocking=True)\n        yb = yb.to(DEVICE, non_blocking=True)\n\n        opt.zero_grad()\n        # forward with autocast if AMP enabled\n        with torch.cuda.amp.autocast(enabled=use_amp):\n            logits = model(xb)\n            loss = criterion(logits, yb)\n\n        # scale backward + step when using AMP\n        if use_amp:\n            scaler.scale(loss).backward()\n            scaler.step(opt)\n            scaler.update()\n        else:\n            loss.backward()\n            opt.step()\n\n        batch_loss = loss.item()\n        train_loss += batch_loss * xb.size(0)\n        n_samples += xb.size(0)\n        pbar.set_postfix({\"batch_loss\": f\"{batch_loss:.4f}\"})\n\n    avg_train_loss = train_loss / max(1, n_samples)\n\n    # validation\n    model.eval()\n    tot, ok = 0, 0\n    with torch.no_grad():\n        for xb, yb in tqdm(val_dl, desc=f\"Epoch {epoch:02d} [val]  \", leave=False):\n            xb = xb.to(DEVICE, non_blocking=True)\n            yb = yb.to(DEVICE, non_blocking=True)\n            with torch.cuda.amp.autocast(enabled=use_amp):\n                preds = model(xb).argmax(dim=1)\n            tot += yb.size(0)\n            ok += (preds == yb).sum().item()\n\n    val_acc = ok / tot if tot > 0 else 0.0\n    print(f\"Epoch {epoch:02d}: train_loss={avg_train_loss:.4f}  val_acc={val_acc:.4f}\")\n\n    ckpt = {\n        \"epoch\": epoch,\n        \"model_state\": model.state_dict(),\n        \"optimizer_state\": opt.state_dict(),\n        \"val_acc\": val_acc,\n        \"label_to_idx\": label_to_idx\n    }\n    torch.save(ckpt, MODEL_DIR / f\"chk_epoch{epoch:03d}.pth\")\n    if val_acc > best_val:\n        best_val = val_acc\n        torch.save(ckpt, MODEL_DIR / \"best_model.pth\")\n\nprint(\"Training finished. Best val_acc:\", best_val)\n","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null}]}